{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "env-jupyterbook",
      "language": "python",
      "name": "env-jupyterbook"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "EE 298 M-MOZQ-Assignment-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ZUqVeFUwUy"
      },
      "source": [
        "# Neural Network from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVSsT7nyUwU4"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmIC1DWqUwU5"
      },
      "source": [
        "mu, sigma = 0, 0.1 # mean and standard deviation"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoY-ESD_UwU6"
      },
      "source": [
        "# For the given mean and std, draw  1,000,000 random samples from ‚àí2ùúé, +2ùúé to build a dataset\n",
        "s = np.random.normal(mu, sigma, 1000000)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Qd_BF0l9UwU6",
        "outputId": "6a1324ea-64cb-46c1-d85d-58dafb79fa37"
      },
      "source": [
        "nbin = 1000\n",
        "count, bins, ignored = plt.hist(s, nbin, density=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPGYROfaUwU9"
      },
      "source": [
        "# Split the dataset into train (90%) and test(10%) \n",
        "x = np.asarray(bins[:-1])[..., np.newaxis]\n",
        "y = np.asarray(count)[..., np.newaxis]\n",
        "X = np.concatenate((x, y), axis=1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVoS7vYXUwU-"
      },
      "source": [
        "np.random.shuffle(X)\n",
        "slicesplit = int(0.9*nbin)\n",
        "training, test = X[:slicesplit,:], X[slicesplit:,:]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVoDXNSKXHtq"
      },
      "source": [
        "# Using pure Python3 and Numpy, build a 3-layer neural networks:\n",
        "# - Layers: {1 ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí 1 ‚àí ùë†ùëñùëîùëöùëúùëñùëë} \n",
        "# - Initialize the weights using a Gaussian distribution with zero mean and std=0.01."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YDvfBfeUwU_"
      },
      "source": [
        "# Class Definitions\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.01*np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "    \n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        \n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "        \n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable, \n",
        "        # let's make a copy of values first \n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Zero gradient where input values were negative \n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "class Activation_Sigmoid:\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        self.output = 1 / (1 + np.exp(-inputs))\n",
        "        \n",
        "    def backward(self, dvalues):\n",
        "    # Derivative - calculates from output of the sigmoid function \n",
        "        self.dinputs = dvalues*(1 - self.output)*self.output\n",
        "    \n",
        "    \n",
        "class Loss:\n",
        "    # Calculates the data losses \n",
        "    # given model output and ground truth values \n",
        "    def calculate(self, output, y):\n",
        "        # Calculate sample losses \n",
        "        sample_losses = self.forward(output, y)\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "        # Return loss \n",
        "        return data_loss\n",
        "    \n",
        "\n",
        "class Loss_MeanSquaredError(Loss): \n",
        "    # Mean Squared Error loss\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true): # Calculate loss\n",
        "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1) \n",
        "        # Return losses\n",
        "        return sample_losses\n",
        "    \n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of outputs in every sample\n",
        "        # We'll use the first sample to count them \n",
        "        outputs = len(dvalues[0])\n",
        "        \n",
        "        # Gradient on values\n",
        "        self.dinputs = -2*(y_true - dvalues)/outputs \n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs/samples\n",
        "\n",
        "        \n",
        "class Optimizer_SGD:\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate = 0.1): \n",
        "        self.learning_rate = learning_rate\n",
        "    \n",
        "    def update_params(self, layer):\n",
        "        layer.weights += -self.learning_rate * layer.dweights\n",
        "        layer.biases += -self.learning_rate * layer.dbiases\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZBn-8uUUwVB"
      },
      "source": [
        "# Network Construction\n",
        "dense1 = Layer_Dense(1, 64)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(64, 64)\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "dense3 = Layer_Dense(64, 1)\n",
        "activation3 = Activation_Sigmoid() \n",
        "\n",
        "loss_function = Loss_MeanSquaredError()\n",
        "optimizer = Optimizer_SGD(learning_rate = 0.1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_rz951oUwVB",
        "outputId": "f1f0c664-a43f-40ec-8428-97842fdc3dc0"
      },
      "source": [
        "# Train for 20 epochs and evaluate the performance of your network\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS): \n",
        "    \n",
        "    # Training\n",
        "    avetrainloss = []\n",
        "    for row in training: #Batch size = 1\n",
        "        X = row[0]\n",
        "        y = row[1]\n",
        "        \n",
        "        # Forward pass\n",
        "        dense1.forward(X)\n",
        "        activation1.forward(dense1.output)\n",
        "\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "\n",
        "        dense3.forward(activation2.output)\n",
        "        activation3.forward(dense3.output)\n",
        "\n",
        "        trainloss = loss_function.calculate(activation3.output, y)\n",
        "\n",
        "        # Backward pass \n",
        "        loss_function.backward(activation3.output, y)\n",
        "        \n",
        "        activation3.backward(loss_function.dinputs) \n",
        "        dense3.backward(activation3.dinputs) \n",
        "        \n",
        "        activation2.backward(dense3.dinputs) \n",
        "        dense2.backward(activation2.dinputs) \n",
        "        \n",
        "        activation1.backward(dense2.dinputs) \n",
        "        dense1.backward(activation1.dinputs)\n",
        "\n",
        "        # Update weights and biases\n",
        "        optimizer.update_params(dense1) \n",
        "        optimizer.update_params(dense2)\n",
        "        optimizer.update_params(dense3)\n",
        "        \n",
        "        avetrainloss.append(trainloss)\n",
        "    \n",
        "    # Validation\n",
        "    avetestloss = []\n",
        "    for row in test:\n",
        "        #Batch size = 1\n",
        "        X = row[0]\n",
        "        y = row[1]\n",
        "        \n",
        "        # Forward pass\n",
        "        dense1.forward(X)\n",
        "        activation1.forward(dense1.output)\n",
        "\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "\n",
        "        dense3.forward(activation2.output)\n",
        "        activation3.forward(dense3.output)\n",
        "\n",
        "        \n",
        "        # Loss\n",
        "        testloss = loss_function.calculate(activation3.output, y)\n",
        "        \n",
        "        avetestloss.append(testloss)\n",
        "        \n",
        "    print(\"Epoch: {}, Train loss: {}, Test loss {}\".format(epoch,np.mean(avetrainloss),np.mean(avetestloss))) \n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 1.8955272017930878, Test loss 1.9030430629340924\n",
            "Epoch: 1, Train loss: 1.868596046115305, Test loss 1.9008074261022925\n",
            "Epoch: 2, Train loss: 1.8665915690251604, Test loss 1.9000758673084732\n",
            "Epoch: 3, Train loss: 1.8657512414758703, Test loss 1.8997165438023849\n",
            "Epoch: 4, Train loss: 1.8652898261699875, Test loss 1.8995066636822378\n",
            "Epoch: 5, Train loss: 1.8650021477881311, Test loss 1.899371330577645\n",
            "Epoch: 6, Train loss: 1.8648084654652777, Test loss 1.8992781548880964\n",
            "Epoch: 7, Train loss: 1.8646709750201504, Test loss 1.8992108974952497\n",
            "Epoch: 8, Train loss: 1.864569432715824, Test loss 1.899160552345066\n",
            "Epoch: 9, Train loss: 1.8644920758979016, Test loss 1.899121762350519\n",
            "Epoch: 10, Train loss: 1.8644316409362278, Test loss 1.8990911648103028\n",
            "Epoch: 11, Train loss: 1.8643834205423924, Test loss 1.8990665408590652\n",
            "Epoch: 12, Train loss: 1.8643442714575935, Test loss 1.8990464074733353\n",
            "Epoch: 13, Train loss: 1.8643119969184192, Test loss 1.8990296990801818\n",
            "Epoch: 14, Train loss: 1.8642850359458432, Test loss 1.8990156602739896\n",
            "Epoch: 15, Train loss: 1.864262253394112, Test loss 1.8990037349889801\n",
            "Epoch: 16, Train loss: 1.864242803027948, Test loss 1.8989935071487454\n",
            "Epoch: 17, Train loss: 1.8642260460661495, Test loss 1.898984658644301\n",
            "Epoch: 18, Train loss: 1.8642114920890622, Test loss 1.8989769442599829\n",
            "Epoch: 19, Train loss: 1.864198758768603, Test loss 1.8989701719743117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GHUFZgbXSrs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}