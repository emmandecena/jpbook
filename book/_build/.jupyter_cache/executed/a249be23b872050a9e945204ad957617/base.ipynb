{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CVSsT7nyUwU4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KmIC1DWqUwU5"
   },
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CoY-ESD_UwU6"
   },
   "outputs": [],
   "source": [
    "# For the given mean and std, draw  1,000,000 random samples from ‚àí2ùúé, +2ùúé to build a dataset\n",
    "s = np.random.normal(mu, sigma, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Qd_BF0l9UwU6",
    "outputId": "6a1324ea-64cb-46c1-d85d-58dafb79fa37"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbin = 1000\n",
    "count, bins, ignored = plt.hist(s, nbin, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mPGYROfaUwU9"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train (90%) and test(10%) \n",
    "x = np.asarray(bins[:-1])[..., np.newaxis]\n",
    "y = np.asarray(count)[..., np.newaxis]\n",
    "X = np.concatenate((x, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GVoS7vYXUwU-"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(X)\n",
    "slicesplit = int(0.9*nbin)\n",
    "training, test = X[:slicesplit,:], X[slicesplit:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MVoDXNSKXHtq"
   },
   "outputs": [],
   "source": [
    "# Using pure Python3 and Numpy, build a 3-layer neural networks:\n",
    "# - Layers: {1 ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí 1 ‚àí ùë†ùëñùëîùëöùëúùëñùëë} \n",
    "# - Initialize the weights using a Gaussian distribution with zero mean and std=0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5YDvfBfeUwU_"
   },
   "outputs": [],
   "source": [
    "# Class Definitions\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "    # Derivative - calculates from output of the sigmoid function \n",
    "        self.dinputs = dvalues*(1 - self.output)*self.output\n",
    "    \n",
    "    \n",
    "class Loss:\n",
    "    # Calculates the data losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss \n",
    "        return data_loss\n",
    "    \n",
    "\n",
    "class Loss_MeanSquaredError(Loss): \n",
    "    # Mean Squared Error loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true): # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1) \n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2*(y_true - dvalues)/outputs \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate = 0.1): \n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qZBn-8uUUwVB"
   },
   "outputs": [],
   "source": [
    "# Network Construction\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "activation3 = Activation_Sigmoid() \n",
    "\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_rz951oUwVB",
    "outputId": "f1f0c664-a43f-40ec-8428-97842fdc3dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.8713897446185137, Test loss 1.8352309081299791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.8458849419701862, Test loss 1.837242097022344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 1.8440920276615156, Test loss 1.8382616557603193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 1.8433408619116616, Test loss 1.8388921651452335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.8429196210901277, Test loss 1.8393242646608912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 1.8426489643322799, Test loss 1.8396389059044977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 1.8424606853990588, Test loss 1.8398774386572163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 1.8423226551801477, Test loss 1.8400635624495474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 1.842217598685461, Test loss 1.8402120555555852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 1.8421353241399492, Test loss 1.8403325325324296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 1.8420693869286362, Test loss 1.8404318437476124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 1.842015624797165, Test loss 1.8405146501682828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 1.841971070204426, Test loss 1.8405844052986147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 1.841933642080881, Test loss 1.8406437314558017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train loss: 1.841901855124302, Test loss 1.8406945810317663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 1.8418745715074147, Test loss 1.840738468345467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train loss: 1.8418509326057138, Test loss 1.8407765620391343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train loss: 1.8418302859477473, Test loss 1.8408098246705304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train loss: 1.841812111926405, Test loss 1.8408389906938454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train loss: 1.8417960025203817, Test loss 1.8408646708374985\n"
     ]
    }
   ],
   "source": [
    "# Train for 20 epochs and evaluate the performance of your network\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    # Training\n",
    "    avetrainloss = []\n",
    "    for row in training: #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        trainloss = loss_function.calculate(activation3.output, y)\n",
    "\n",
    "        # Backward pass \n",
    "        loss_function.backward(activation3.output, y)\n",
    "        \n",
    "        activation3.backward(loss_function.dinputs) \n",
    "        dense3.backward(activation3.dinputs) \n",
    "        \n",
    "        activation2.backward(dense3.dinputs) \n",
    "        dense2.backward(activation2.dinputs) \n",
    "        \n",
    "        activation1.backward(dense2.dinputs) \n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update weights and biases\n",
    "        optimizer.update_params(dense1) \n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        \n",
    "        avetrainloss.append(trainloss)\n",
    "    \n",
    "    # Validation\n",
    "    avetestloss = []\n",
    "    for row in test:\n",
    "        #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        \n",
    "        # Loss\n",
    "        testloss = loss_function.calculate(activation3.output, y)\n",
    "        \n",
    "        avetestloss.append(testloss)\n",
    "        \n",
    "    print(\"Epoch: {}, Train loss: {}, Test loss {}\".format(epoch,np.mean(avetrainloss),np.mean(avetestloss))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GHUFZgbXSrs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EE 298 M-MOZQ-Assignment-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env-jupyterbook",
   "language": "python",
   "name": "env-jupyterbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}