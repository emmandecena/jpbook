{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_ZUqVeFUwUy"
   },
   "source": [
    "# Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CVSsT7nyUwU4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KmIC1DWqUwU5"
   },
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CoY-ESD_UwU6"
   },
   "outputs": [],
   "source": [
    "# For the given mean and std, draw  1,000,000 random samples from ‚àí2ùúé, +2ùúé to build a dataset\n",
    "s = np.random.normal(mu, sigma, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Qd_BF0l9UwU6",
    "outputId": "6a1324ea-64cb-46c1-d85d-58dafb79fa37"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Volumes/data/projects/site-portfolio/book/_build/jupyter_execute/docs/neuralnetworks/local/annfromscratch_4_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbin = 1000\n",
    "count, bins, ignored = plt.hist(s, nbin, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mPGYROfaUwU9"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train (90%) and test(10%) \n",
    "x = np.asarray(bins[:-1])[..., np.newaxis]\n",
    "y = np.asarray(count)[..., np.newaxis]\n",
    "X = np.concatenate((x, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GVoS7vYXUwU-"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(X)\n",
    "slicesplit = int(0.9*nbin)\n",
    "training, test = X[:slicesplit,:], X[slicesplit:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MVoDXNSKXHtq"
   },
   "outputs": [],
   "source": [
    "# Using pure Python3 and Numpy, build a 3-layer neural networks:\n",
    "# - Layers: {1 ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí 1 ‚àí ùë†ùëñùëîùëöùëúùëñùëë} \n",
    "# - Initialize the weights using a Gaussian distribution with zero mean and std=0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5YDvfBfeUwU_"
   },
   "outputs": [],
   "source": [
    "# Class Definitions\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "    # Derivative - calculates from output of the sigmoid function \n",
    "        self.dinputs = dvalues*(1 - self.output)*self.output\n",
    "    \n",
    "    \n",
    "class Loss:\n",
    "    # Calculates the data losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss \n",
    "        return data_loss\n",
    "    \n",
    "\n",
    "class Loss_MeanSquaredError(Loss): \n",
    "    # Mean Squared Error loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true): # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1) \n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2*(y_true - dvalues)/outputs \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate = 0.1): \n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qZBn-8uUUwVB"
   },
   "outputs": [],
   "source": [
    "# Network Construction\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "activation3 = Activation_Sigmoid() \n",
    "\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_rz951oUwVB",
    "outputId": "f1f0c664-a43f-40ec-8428-97842fdc3dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.9370503876607117, Test loss 1.6040619292309284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.9071385369342608, Test loss 1.6081042863440225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 1.904803181824408, Test loss 1.6098595963304525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 1.9038146575884596, Test loss 1.6108569532442807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.9032646895024627, Test loss 1.6114954982056053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 1.902918182896755, Test loss 1.6119332520517984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 1.9026833517200556, Test loss 1.612247508418499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 1.9025161154786148, Test loss 1.6124810134773975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 1.9023925461881397, Test loss 1.6126593569458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 1.902298538610807, Test loss 1.6127987058399162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 1.9022252894924134, Test loss 1.6129097082525872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 1.9021670562488533, Test loss 1.612999620196231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 1.9021199575867564, Test loss 1.613073519056885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 1.9020812924297135, Test loss 1.6131350414760874\n",
      "Epoch: 14, Train loss: 1.9020491337012342, Test loss 1.6131868440339765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 1.9020220761914324, Test loss 1.6132309100484674\n",
      "Epoch: 16, Train loss: 1.901999076820736, Test loss 1.6132687370503676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train loss: 1.9019793472485003, Test loss 1.6133014777252925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train loss: 1.9019622817850683, Test loss 1.613330027567842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train loss: 1.9019474104432805, Test loss 1.613355091634702\n"
     ]
    }
   ],
   "source": [
    "# Train for 20 epochs and evaluate the performance of your network\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    # Training\n",
    "    avetrainloss = []\n",
    "    for row in training: #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        trainloss = loss_function.calculate(activation3.output, y)\n",
    "\n",
    "        # Backward pass \n",
    "        loss_function.backward(activation3.output, y)\n",
    "        \n",
    "        activation3.backward(loss_function.dinputs) \n",
    "        dense3.backward(activation3.dinputs) \n",
    "        \n",
    "        activation2.backward(dense3.dinputs) \n",
    "        dense2.backward(activation2.dinputs) \n",
    "        \n",
    "        activation1.backward(dense2.dinputs) \n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update weights and biases\n",
    "        optimizer.update_params(dense1) \n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        \n",
    "        avetrainloss.append(trainloss)\n",
    "    \n",
    "    # Validation\n",
    "    avetestloss = []\n",
    "    for row in test:\n",
    "        #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        \n",
    "        # Loss\n",
    "        testloss = loss_function.calculate(activation3.output, y)\n",
    "        \n",
    "        avetestloss.append(testloss)\n",
    "        \n",
    "    print(\"Epoch: {}, Train loss: {}, Test loss {}\".format(epoch,np.mean(avetrainloss),np.mean(avetestloss))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GHUFZgbXSrs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EE 298 M-MOZQ-Assignment-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env-jupyterbook",
   "language": "python",
   "name": "env-jupyterbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}