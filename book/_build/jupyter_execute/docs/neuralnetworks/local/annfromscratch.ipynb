{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_ZUqVeFUwUy"
   },
   "source": [
    "# Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CVSsT7nyUwU4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KmIC1DWqUwU5"
   },
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CoY-ESD_UwU6"
   },
   "outputs": [],
   "source": [
    "# For the given mean and std, draw  1,000,000 random samples from ‚àí2ùúé, +2ùúé to build a dataset\n",
    "s = np.random.normal(mu, sigma, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Qd_BF0l9UwU6",
    "outputId": "6a1324ea-64cb-46c1-d85d-58dafb79fa37"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Volumes/data/projects/site-portfolio/book/_build/jupyter_execute/docs/neuralnetworks/local/annfromscratch_4_0.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbin = 1000\n",
    "count, bins, ignored = plt.hist(s, nbin, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mPGYROfaUwU9"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train (90%) and test(10%) \n",
    "x = np.asarray(bins[:-1])[..., np.newaxis]\n",
    "y = np.asarray(count)[..., np.newaxis]\n",
    "X = np.concatenate((x, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GVoS7vYXUwU-"
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(X)\n",
    "slicesplit = int(0.9*nbin)\n",
    "training, test = X[:slicesplit,:], X[slicesplit:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MVoDXNSKXHtq"
   },
   "outputs": [],
   "source": [
    "# Using pure Python3 and Numpy, build a 3-layer neural networks:\n",
    "# - Layers: {1 ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí 1 ‚àí ùë†ùëñùëîùëöùëúùëñùëë} \n",
    "# - Initialize the weights using a Gaussian distribution with zero mean and std=0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5YDvfBfeUwU_"
   },
   "outputs": [],
   "source": [
    "# Class Definitions\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "    # Derivative - calculates from output of the sigmoid function \n",
    "        self.dinputs = dvalues*(1 - self.output)*self.output\n",
    "    \n",
    "    \n",
    "class Loss:\n",
    "    # Calculates the data losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss \n",
    "        return data_loss\n",
    "    \n",
    "\n",
    "class Loss_MeanSquaredError(Loss): \n",
    "    # Mean Squared Error loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true): # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1) \n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2*(y_true - dvalues)/outputs \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate = 0.1): \n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qZBn-8uUUwVB"
   },
   "outputs": [],
   "source": [
    "# Network Construction\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "activation3 = Activation_Sigmoid() \n",
    "\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_rz951oUwVB",
    "outputId": "f1f0c664-a43f-40ec-8428-97842fdc3dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.9185083589903889, Test loss 1.3953661006415012\n",
      "Epoch: 1, Train loss: 1.8918284087842694, Test loss 1.4015183905253117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 1.8897122266193558, Test loss 1.4042082279847485\n",
      "Epoch: 3, Train loss: 1.8888549711550553, Test loss 1.4057315208647487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.8883938351219562, Test loss 1.4067032612478336\n",
      "Epoch: 5, Train loss: 1.8881099389457172, Test loss 1.4073689106089529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 1.8879203003676241, Test loss 1.4078478209847478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 1.8877862925411548, Test loss 1.4082052684958233\n",
      "Epoch: 8, Train loss: 1.8876875653011451, Test loss 1.4084799279056244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 1.887612429489505, Test loss 1.4086960404479258\n",
      "Epoch: 10, Train loss: 1.8875537100290194, Test loss 1.40886941680104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 1.8875068065010587, Test loss 1.4090108328503723\n",
      "Epoch: 12, Train loss: 1.8874686700231784, Test loss 1.4091278831762337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 1.88743716270679, Test loss 1.4092259444875195\n",
      "Epoch: 14, Train loss: 1.8874107731047691, Test loss 1.4093089858398604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 1.8873884018773748, Test loss 1.4093799469031427\n",
      "Epoch: 16, Train loss: 1.887369239658176, Test loss 1.4094411343227484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train loss: 1.8873526670202558, Test loss 1.4094942862658835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train loss: 1.8873382134916794, Test loss 1.409540741426852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train loss: 1.88732550621612, Test loss 1.4095815907530451\n"
     ]
    }
   ],
   "source": [
    "# Train for 20 epochs and evaluate the performance of your network\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    # Training\n",
    "    avetrainloss = []\n",
    "    for row in training: #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        trainloss = loss_function.calculate(activation3.output, y)\n",
    "\n",
    "        # Backward pass \n",
    "        loss_function.backward(activation3.output, y)\n",
    "        \n",
    "        activation3.backward(loss_function.dinputs) \n",
    "        dense3.backward(activation3.dinputs) \n",
    "        \n",
    "        activation2.backward(dense3.dinputs) \n",
    "        dense2.backward(activation2.dinputs) \n",
    "        \n",
    "        activation1.backward(dense2.dinputs) \n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update weights and biases\n",
    "        optimizer.update_params(dense1) \n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        \n",
    "        avetrainloss.append(trainloss)\n",
    "    \n",
    "    # Validation\n",
    "    avetestloss = []\n",
    "    for row in test:\n",
    "        #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        \n",
    "        # Loss\n",
    "        testloss = loss_function.calculate(activation3.output, y)\n",
    "        \n",
    "        avetestloss.append(testloss)\n",
    "        \n",
    "    print(\"Epoch: {}, Train loss: {}, Test loss {}\".format(epoch,np.mean(avetrainloss),np.mean(avetestloss))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GHUFZgbXSrs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EE 298 M-MOZQ-Assignment-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env-jupyterbook",
   "language": "python",
   "name": "env-jupyterbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}